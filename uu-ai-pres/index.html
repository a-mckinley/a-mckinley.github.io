<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>AI Use Cases In Action: Background and Practical Implementation Considerations for Vector Search and RAG</title>

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/reset.css">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/reveal.css">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/theme/beige.css">
		<style>
			.reveal .slides { font-size: 0.9em; }
			.reveal .footer { 
				position: absolute; 
				bottom: 10px; 
				left: 10px; 
				right: 10px; 
				font-size: 0.5em; 
				z-index: 1000;
				padding: 5px;
				display: flex;
				justify-content: space-between;
				align-items: center;
			}
			.reveal .footer .logo { 
				width: 120px; 
				height: auto;
			}
			.reveal .slide-number {
				position: static;
			}
		</style>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/pako/2.0.4/pako.min.js"></script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Title Slide -->
				<section data-markdown>
					<textarea data-template>
						# AI Use Cases In Action
						### Background and Practical Implementation Considerations for Vector Search and RAG

						Alastair McKinley

						Head of Data Engineering 
						and AI
						@Scileads
					</textarea>
				</section>

				<!-- Bio -->
				<section data-markdown>
					<textarea data-template>
						## Bio

						- Data and AI Engineer with 15 years of experience
						- Head of Data Engineering and AI at Scileads
						- Former CTO at Analytics Engines
						- Implemented solutions in BI, predictive analytics, search
						- Recent focus on LLM-based solutions for unstructured data
					</textarea>
				</section>

				<!-- Introduction to embeddings and RAG -->
				<section>
					<section data-markdown>
						<textarea data-template>
							# Introduction to embeddings and RAG
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## What are embeddings?

							- Dense vector representations of data
							- Capture semantic meaning in high-dimensional space
							- Enable efficient similarity calculations
							- Fundamental to modern NLP and AI applications
							- Basis for various downstream tasks
						</textarea>
					</section>
					<section>
						<h2>Transformer Neural Network and Embedding</h2>
						<img src="" alt="transformerEmbedding" class="kroki-diagram">
					</section>
					<section data-markdown>
						<textarea data-template>
							## How are Embeddings Generated by LLMs?

							- Input data is tokenized into smaller units
							- Tokens processed through multiple transformer layers
							- Self-attention mechanisms capture contextual information
							- Final layer outputs dense vector representation
							- Dimensionality reduction may be applied for efficiency
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## What do Embeddings represent?

							- Semantic and contextual information of input data
							- Relationships between words or concepts
							- Data in a continuous vector space
							- Basis for mathematical operations on language
							- Preservation of similarity and analogy relationships
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## How can Embeddings be used?

							- Semantic search and information retrieval
							- Document classification and clustering
							- Recommendation systems
							- Language translation
							- Anomaly detection in text data
						</textarea>
					</section>
				</section>

				<!-- Applications of embeddings -->
				<section>
					<section data-markdown>
						<textarea data-template>
							# Applications of embeddings
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## Semantic Text Search

							- Understands query intent and context
							- Matches conceptually similar documents
							- Improves search accuracy and relevance
							- Handles synonyms and related concepts naturally
							- Supports multilingual search capabilities
						</textarea>
					</section>
					<section>
						<h2>Semantic Text Search Process</h2>
						<img src="" alt="semanticSearch" class="kroki-diagram">
					</section>
					<section data-markdown>
						<textarea data-template>
							## Image Similarity Search

							- Generates embeddings from image features
							- Enables content-based image retrieval
							- Finds visually similar images efficiently
							- Supports tasks like reverse image search
							- Useful in e-commerce and digital asset management
						</textarea>
					</section>
					<section>
						<h2>Image Similarity Search Process</h2>
						<img src="" alt="imageSimilarity" class="kroki-diagram">
					</section>
				</section>

				<!-- Retrieval Augmented Generation and Embeddings -->
				<section>
					<section data-markdown>
						<textarea data-template>
							# Retrieval Augmented Generation and Embeddings
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## RAG Overview

							- Combines retrieval systems with generative AI
							- Enhances LLM responses with external knowledge
							- Improves accuracy and reduces hallucinations
							- Enables real-time information updates
							- Maintains privacy by not encoding all data in model
						</textarea>
					</section>
					<section>
						<h2>RAG Process Overview</h2>
						<img src="" alt="ragOverview" class="kroki-diagram">
					</section>
					<section data-markdown>
						<textarea data-template>
							## Embeddings in RAG

							- Used to index and retrieve relevant documents
							- Enable efficient similarity search in large datasets
							- Improve context selection for LLM prompts
							- Enhance the relevance of generated responses
							- Support dynamic knowledge integration in AI systems
						</textarea>
					</section>
				</section>

				<!-- Approximate nearest neighbour search -->
				<section>
					<section data-markdown>
						<textarea data-template>
							# Approximate nearest neighbour search
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## IVFFlat

							- Inverted File Flat index structure
							- Divides vector space into clusters
							- Speeds up search by limiting to relevant clusters
							- Trade-off between speed and accuracy
							- Effective for medium-sized datasets
						</textarea>
					</section>
					<section>
						<h2>IVFFlat Structure</h2>
						<img src="" alt="ivfFlat" class="kroki-diagram">
					</section>
					<section data-markdown>
						<textarea data-template>
							## HNSW

							- Hierarchical Navigable Small World graphs
							- Builds multi-layer graph structure for search
							- Offers logarithmic search complexity
							- Provides excellent speed-accuracy trade-off
							- Memory-intensive but highly efficient
						</textarea>
					</section>
					<section>
						<h2>HNSW Structure</h2>
						<img src="" alt="hnsw" class="kroki-diagram">
					</section>
					<section data-markdown>
						<textarea data-template>
							## DiskANN

							- Disk-based Approximate Nearest Neighbor search
							- Designed for large-scale, out-of-memory datasets
							- Combines graph-based indexing with disk storage
							- Offers sub-linear search time complexity
							- Balances performance and memory constraints
						</textarea>
					</section>
					<section>
						<h2>DiskANN Structure</h2>
						<img src="" alt="diskANN" class="kroki-diagram">
					</section>
				</section>

				<!-- More Practical considerations -->
				<section>
					<section data-markdown>
						<textarea data-template>
							# More Practical considerations
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## Embedding generation

							- Choice of embedding model affects performance
							- Consider domain-specific vs general-purpose models
							- Evaluate trade-offs between accuracy and compute cost
							- Batch processing for efficiency in large datasets
							- Keep up with evolving embedding techniques
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## Search Quality

							- Evaluate recall and precision metrics
							- Consider the impact of vector dimensionality
							- Test with diverse query types and edge cases
							- Implement relevance feedback mechanisms
							- Regularly update and fine-tune search algorithms
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## Approximate nearest neighbour search

							- Balance between search speed and accuracy
							- Consider dataset size and update frequency
							- Evaluate hardware requirements (CPU, GPU, memory)
							- Implement efficient index update strategies
							- Monitor and optimize search latency
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## Matryoshka Representation Learning

							- Nested embedding representation technique
							- Allows for adaptive dimensionality
							- Improves efficiency in multi-scale similarity search
							- Enables flexible trade-offs between speed and accuracy
							- Useful for handling diverse query complexities
						</textarea>
					</section>
					<section>
						<h2>Matryoshka Representation</h2>
						<img src="" alt="matryoshka" class="kroki-diagram">
					</section>
					<section data-markdown>
						<textarea data-template>
							## Binary Quantization

							- Converts continuous embeddings to binary format
							- Significantly reduces storage requirements
							- Enables faster similarity computations
							- Trade-off between compression and accuracy
							- Useful for large-scale, resource-constrained applications
						</textarea>
					</section>
					<section>
						<h2>Binary Quantization Process</h2>
						<img src="" alt="binaryQuantization" class="kroki-diagram">
					</section>
				</section>

				<!-- Q&A -->
				<section data-markdown>
					<textarea data-template>
						# Q&A

						Thank you for your attention!
						Any questions?
					</textarea>
				</section>
			</div>
			<div class="footer">
				<img src="images/logo.png" alt="Scileads Logo" class="logo">
				<span class="slide-number"></span>
				<span class="presentation-title">AI Use Cases In Action: Vector Search and RAG</span>
			</div>
		</div>

		<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/reveal.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/plugin/markdown/markdown.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/plugin/highlight/highlight.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/plugin/math/math.js"></script>
		<script>
			// Diagram definitions
			const diagrams = {
				transformerEmbedding: {
					type: 'graphviz',
					content: `
						digraph G {
							rankdir=LR;
							node [shape=box];
							Input -> Tokenizer;
							Tokenizer -> "Transformer Layers";
							"Transformer Layers" -> "Dense Vector";
							"Dense Vector" -> Output;
						}
					`
				},
				semanticSearch: {
					type: 'mermaid',
					content: `
						graph TD
						A[User Query] --> B[Embedding Generation]
						B --> C[Vector Search]
						C --> D[Retrieve Similar Documents]
						D --> E[Rank Results]
						E --> F[Present to User]
					`
				},
				imageSimilarity: {
					type: 'mermaid',
					content: `
						graph TD
						A[Input Image] --> B[Feature Extraction]
						B --> C[Embedding Generation]
						C --> D[Vector Search]
						D --> E[Retrieve Similar Images]
						E --> F[Present Results]
					`
				},
				ragOverview: {
					type: 'mermaid',
					content: `
						graph TD
						A[User Query] --> B[Embedding Generation]
						B --> C[Vector Search]
						C --> D[Retrieve Relevant Documents]
						D --> E[Augment LLM Prompt]
						E --> F[Generate Response]
						F --> G[Present to User]
					`
				},
				ivfFlat: {
					type: 'mermaid',
					content: `
						graph TD
						A[Vector Space] --> B[Clustering]
						B --> C[Inverted Index]
						C --> D[Search within Relevant Clusters]
					`
				},
				hnsw: {
					type: 'mermaid',
					content: `
						graph TD
						A[Multi-layer Graph] --> B[Entry Point]
						B --> C[Navigate Layers]
						C --> D[Refine Search]
						D --> E[Nearest Neighbors]
					`
				},
				diskANN: {
					type: 'mermaid',
					content: `
						graph TD
						A[Large Dataset] --> B[Graph-based Index]
						B --> C[Disk Storage]
						C --> D[In-memory Graph]
						D --> E[Efficient Search]
					`
				},
				matryoshka: {
					type: 'mermaid',
					content: `
						graph TD
						A[Full Embedding] --> B[Nested Subsets]
						B --> C[Adaptive Search]
						C --> D[Multi-scale Similarity]
					`
				},
				binaryQuantization: {
					type: 'mermaid',
					content: `
						graph TD
						A[Continuous Embedding] --> B[Thresholding]
						B --> C[Binary Representation]
						C --> D[Compact Storage]
						D --> E[Fast Similarity Computation]
					`
				}
			};

			// Function to encode diagram to URL-safe string
			function encodeKrokiDiagram(content) {
				const encoder = new TextEncoder();
				const data = encoder.encode(content);
				const compressed = pako.deflate(data, { level: 9 });
				return btoa(String.fromCharCode.apply(null, compressed))
					.replace(/\+/g, '-')
					.replace(/\//g, '_');
			}

			Reveal.initialize({
				hash: true,
				plugins: [ RevealMarkdown, RevealHighlight, RevealMath.KaTeX ],
				slideNumber: 'c/t',
				transition: 'slide',
				autoAnimateDuration: 0.8,
				autoAnimateEasing: 'ease-in-out',
				autoAnimateUnmatched: false,
				fragmentInURL: true,
				embedded: false,
				help: true,
				center: true,
				touch: true,
				loop: false,
				rtl: false,
				navigationMode: 'default',
				shuffle: false,
				mouseWheel: false,
				display: 'block'
			});

			// After Reveal.js initialization, replace diagram placeholders with actual diagrams
			Reveal.on('ready', event => {
				document.querySelectorAll('img.kroki-diagram').forEach(img => {
					const diagramType = img.alt;
					if (diagrams[diagramType]) {
						const { type, content } = diagrams[diagramType];
						const encodedDiagram = encodeKrokiDiagram(content);
						img.src = `https://kroki.io/${type}/svg/${encodedDiagram}`;
					}
				});
			});
		</script>
	</body>
</html>

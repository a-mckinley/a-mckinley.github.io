<!doctype html>
<!--
Approx structure of the presentation:
1. Introduction to embeddings and RAG
    1a. What are embeddings?
        - Include a block diagram of a TransfoerNeural Network taking in a piece of data and outputting an embedding.
    1b. How are Embeddings Generated by LLMs?
    1c. What do Embeddings represent?
    1d. How can they be used?
2. Applications of embeddings
    2a. Semantic Text Search
    2b. Image Simalarity Search
3. Retreival Augmented Generation and Embeddings
    3a. RAG Overview
    3b. Embeddings in RAG
4. Approximate nearest neighbour search
    4a. IVFFlat
    4b. HNSW
    4c. DiskANN
5. More Practical considerations
    5a. Embedding generation
    5b. Search Quality
    5c. Approximate nearest neighbour search
    5d. Matryoshka Represenation Learning
    5e. Binary Quantisation
6. Q&A

-->
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>AI Use Cases In Action: Background and Practical Implementation Considerations for Vector Search and RAG</title>

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/reset.css">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/reveal.css">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/theme/beige.css">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
		<style>
			.reveal .slides { font-size: 0.9em; }
			.reveal .footer { 
				position: absolute; 
				bottom: 10px; 
				left: 10px; 
				right: 10px; 
				font-size: 0.5em; 
				z-index: 1000;
				padding: 5px;
				display: flex;
				justify-content: space-between;
				align-items: center;
			}
			.reveal .footer .logo { 
				width: 120px; 
				height: auto;
			}
			.reveal .slide-number {
				position: static;
			}

			.reveal .slides section .wide-code {
				max-width: none;
				font-size: 0.7em; /* Adjust font size if necessary */
			}

			.highlight-red-bold { color: #ff0000; font-weight: bold; text-shadow: 2px 2px 2px rgba(0,0,0,0.3); }
			.highlight-green-bold { color: #00ff00; font-weight: bold; text-shadow: 2px 2px 2px rgba(0,0,0,0.3); }
			.highlight-blue-bold { color: #0000ff; font-weight: bold; text-shadow: 2px 2px 2px rgba(0,0,0,0.3); }
			.highlight-purple-bold { color: #ff00ff; font-weight: bold; text-shadow: 2px 2px 2px rgba(0,0,0,0.3); }
			.highlight-orange-bold { color: #ffa500; font-weight: bold; text-shadow: 2px 2px 2px rgba(0,0,0,0.3); }
			.highlight-bold { font-weight: bold; text-shadow: 2px 2px 2px rgba(0,0,0,0.3); }
		</style>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/pako/2.0.4/pako.min.js"></script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Title Slide -->
				<section data-markdown>
					<textarea data-template>
						# AI Use Cases In Action
						### Background and Practical Implementation Considerations for Vector Search and RAG

						<span class="highlight-bold">Alastair McKinley</span>

						Head of Data Engineering 
						and AI
						@<span class="highlight-orange-bold">Sci</span><span class="highlight-bold">Leads</span>
					</textarea>
				</section>

				<!-- Introduction to embeddings and RAG -->
				<section>
					<section data-markdown>
						<textarea data-template>
							# Introduction to embeddings and RAG
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## What are embeddings?
							- <!-- .element: class="fragment" --> Vector representations of data (any type of data!)
							
							- <!-- .element: class="fragment" --> Typically created with a neural network "Encoder" stage

							- <!-- .element: class="fragment" --> Or use some other hidden state of a decoder only transformer

							- <!-- .element: class="fragment" --> Usually <span class="highlight-blue-bold">very high number of dimensions</span> (e.g. 300-3000)

							- <!-- .element: class="fragment" --> "Hello World" \(\rightarrow [0.1, 0.2, 0.7, \ldots]\)

							- <!-- .element: class="fragment" --> Dog.jpg \(\rightarrow [0.3, 0.4, 0.8, \ldots]\)
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Autoencoder Neural Network and Embedding
							<img src="./images/autoencoder.svg" alt="autoencoderEmbedding" width="800">

							- <!-- .element: class="fragment" --> The latent vector learned by the encoder is the <span class="highlight-blue-bold">embedding</span>
							- <!-- .element: class="fragment" --> Typically trained on some type of <span class="highlight-orange-bold">reconstruction loss</span> (e.g. predict next token - GPT)
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Word Embeddings in Vector Space
							<img src="./images/wordEmbeddings.svg" alt="wordEmbeddings" width="500">

							- <!-- .element: class="fragment" --> *Only 3 dimensions shown for illustration purposes
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							### BERT - One Of The First Useful Embedding Models
							<img src="images/bert_mlm.png" alt="bert" width="500">

							- Credit - https://dvgodoy.github.io/dl-visuals/BERT/
							- Google used BERT (text-to-text) for US search query prediction in 2019
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							## Generating Embeddings On Our Data
							- <!-- .element: class="fragment" --> The data can be anything but typically <span class="highlight-orange-bold">image</span> or <span class="highlight-blue-bold">text</span> data

							- <!-- .element: class="fragment" --> Can be anything up to the maximum input size

							- <!-- .element: class="fragment" --> We may (often) take small fragments of text at a time to create embeddings

							- <!-- .element: class="fragment" --> Input Data: "The quick brown fox jumps over the lazy dog"

							- <!-- .element: class="fragment" --> \( e\text(The\ quick\ brown) \rightarrow [0.1, 0.2, 0.7, \ldots] \)

							- <!-- .element: class="fragment" --> \( e\text(brown\ fox \ jumps) \rightarrow [0.2, 0.3, 0.9, \ldots] \)
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							## What do Embeddings represent?
							
							- <!-- .element: class="fragment" --> Embeddings are lower dimensional learned representations of data

							- <!-- .element: class="fragment" --> A very key property is that nearby embeddings represent similar data

							- <!-- .element: class="fragment" --> For text this <span class="highlight-red-bold">does not depend</span> on the specific words

							- <!-- .element: class="fragment" --> Extremely useful for <span class="highlight-blue-bold">semantic similarity measurement</span>
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Phrase Embeddings in Vector Space
							<img src="./images/phraseEmbeddings.svg" alt="phraseEmbeddings" width="500">

							- <!-- .element: class="fragment" --> The principle of word embeddings applies to virtually any size of text input
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							## What Can We Use Embeddings For?
							- <!-- .element: class="fragment" --> <span class="highlight-blue-bold">Semantic text search</span>
							- <!-- .element: class="fragment" --> <span class="highlight-orange-bold">Image similarity search</span>
							- <!-- .element: class="fragment" --> Document classification and clustering
							- <!-- .element: class="fragment" --> Recommendation systems
							- <!-- .element: class="fragment" --> Anomaly detection in text data
						</textarea>
					</section>
				</section>

				<!-- Applications of embeddings -->
				<section>
					<section data-markdown>
						<textarea data-template>
							# Applications of embeddings
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## Semantic Text Search

							- <!-- .element: class="fragment" --> Uses <span class="highlight-bold">embeddings</span> to find similar text or documents to input query

							- <!-- .element: class="fragment" --> Handles synonyms and related concepts naturally

							- <!-- .element: class="fragment" --> Can support multilingual search capabilities

							- <!-- .element: class="fragment" --> Often used in RAG systems (but also just for search)
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Semantic Text Search Process
							<img src="./images/semanticsearch.svg" alt="semanticSearch">

							- <!-- .element: class="fragment" --> Find the closest vectors to find similar text chunks


							- <!-- .element: class="fragment" --> But how do we do this search for closest embeddings?
						</textarea>	
					</section>
					<section data-markdown>
						<textarea data-template>
							### Vector Nearest Neighbour Search

							- <!-- .element: class="fragment" --> We need to search for the closest vectors


							- <!-- .element: class="fragment" --> Distance can be measured by the <span class="highlight-blue-bold">euclidean distance</span> between vectors

							- <!-- .element: class="fragment" --> Other distance metrics are also possible (cosine, manhattan, jaccard etc.)

						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Vector Euclidean Distance
							<img src="./images/vectoreuclidean.svg" alt="vectorEuclidean" height="400">

							- <!-- .element: class="fragment" --> The euclidean distance between vectors is a measure of their similarity


						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## Vector Nearest Neighbour Search Continued

							- <!-- .element: class="fragment" --> A large database may contain <span class="highlight-orange-bold">millions</span> or <span class="highlight-blue-bold">billions</span> of vectors

							- <!-- .element: class="fragment" --> And euclidean distance is more expensive to compute as dimensions++

							- <!-- .element: class="fragment" --> We need to search for the closest vectors efficiently

							- <!-- .element: class="fragment" --> <span class="highlight-blue-bold">Approximate nearest neighbour</span> (ANN) search algorithms can help
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							## Approximate Nearest Neighbour Search (ANN)

							- <!-- .element: class="fragment" --> A class of algorithms that trade off accuracy for speed

							- <!-- .element: class="fragment" --> The exhaustive approach is <span class="highlight-red-bold">too slow</span> for larger datasets

						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							## Hierarchical Navigable Small World Graphs (HNSW)

							- <!-- .element: class="fragment" --> A graph based approach to <span class="highlight-blue-bold">ANN</span> search

							- <!-- .element: class="fragment" --> Creates a multi-layered graph structure

							- <!-- .element: class="fragment" --> The build process places vectors in layers based on their relative positions

							- <!-- .element: class="fragment" --> Create connections between nodes in different layers and between <span class="highlight-blue-bold">neighbouring nodes</span> on each layer
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							### HNSW Search Process
							<img src="./images/hnsw.svg" alt="hnsw" height="300">

							- <!-- .element: class="fragment" --> Each layer contains a larger fraction of the vectors

							- <!-- .element: class="fragment" --> We find the closest node at the top level and descend through the layers

							- <!-- .element: class="fragment" --> Checking connected nodes at each layer to find the closest node
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							### IVFFlat

							- Divides vector space into <span class="highlight-blue-bold">clusters</span>
							- Speeds up search by limiting to <span class="highlight-bold">relevant clusters</span>
							- But may miss close vectors that are in the edges of other clusters
							- Effective for medium-sized datasets
							- Aim for <span class="highlight-orange-bold">1k-10k</span> vectors per cluster
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							### IVFFlat Search Process
							<img src="./images/ivfflat.svg" alt="ivfflat" height="350">

							- <!-- .element: class="fragment" --> Find the closest cluster centroid exhaustively

							- <!-- .element: class="fragment" --> Search within cluster exhaustively

							- <!-- .element: class="fragment" --> May be combined with PQ for compression within IVFFlat clusters
						</textarea>
					</section>

					<section data-markdown>
						<textarea data-template>
							### ANN Summary
							- <!-- .element: class="fragment" --> <span class="highlight-blue-bold">IVFFlat</span> and <span class="highlight-orange-bold">HNSW</span> help us find vectors efficiently

							- <!-- .element: class="fragment" --> We lose some accuracy but gain speed to retrieve vectors

							- <!-- .element: class="fragment" --> We also <span class="highlight-red-bold">lose write performance</span>, and <span class="highlight-red-bold">increase storage requirements</span>

							- <!-- .element: class="fragment" --> IVFFlat is faster to build, update and delete than HNSW

							- <!-- .element: class="fragment" --> HNSW has higher recall than IVFFlat

							- <!-- .element: class="fragment" --> Some other indexing strategies are gaining popularity (e.g. <span class="highlight-blue-bold">DiskANN</span> and <span class="highlight-blue-bold">Hamming distance</span> searches)

						</textarea>
					</section>

				</section>

				<!-- Retrieval Augmented Generation and Embeddings -->
				<section>
					<section data-markdown>
						<textarea data-template>
							# Retrieval Augmented Generation and Embeddings
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## RAG

							- <!-- .element: class="fragment" --> Modern <span class="highlight-blue-bold">LLMs</span> are amazing, but don't know everything

							- <!-- .element: class="fragment" --> And know very little about our <span class="highlight-orange-bold">internal data</span>

							- <!-- .element: class="fragment" --> RAG uses retrieval to get context from our data or other sources

							- <!-- .element: class="fragment" --> To augment the LLM prompts to "ground" the response

							- <!-- .element: class="fragment" --> Most often, this is using <span class="highlight-blue-bold">embeddings</span> and <span class="highlight-orange-bold">ANN</span> (but not exclusively)

						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### RAG Process Overview
							<img src="./images/rag.svg" alt="rag">

							- <!-- .element: class="fragment" --> This is a very basic RAG diagram
							- <!-- .element: class="fragment" --> Good RAG systems are more complex and have many more components, checks and loops (agents)
							- <!-- .element: class="fragment" --> E.g. Perplexity, Cursor
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## Embeddings in RAG

							- Embeddings are key to enabling <span class="highlight-blue-bold">Semantic Search</span> in RAG

							- Usually required to ensure our LLM recieves <span class="highlight-bold">relevant context</span>
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### RAG Vector Databases

							- <!-- .element: class="fragment" --> There has been an explosion of <span class="highlight-blue-bold">vector databases</span>

							- <!-- .element: class="fragment" --> Many are purpose built for <span class="highlight-bold">RAG</span>

							- <!-- .element: class="fragment" --> E.g. <span class="highlight-blue-bold">Pinecone</span>, <span class="highlight-orange-bold">Chroma</span>, <span class="highlight-blue-bold">Weaviate</span>, <span class="highlight-orange-bold">qdrant</span>, etc.

							- <!-- .element: class="fragment" --> Attempting to optimise storage, ANN and other retrieval optimisations
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### PostgreSQL + PGVector as a Vector DB
							- <!-- .element: class="fragment" --> <span class="highlight-blue-bold">PostgreSQL</span> is a very mature, multi-purpose database


							- <!-- .element: class="fragment" --> <span class="highlight-bold">PGVector</span> is a vector extension for PostgreSQL for ANN


							- <!-- .element: class="fragment" --> Allows us to store and search vectors in the same database as the rest of our data


							- <!-- .element: class="fragment" --> An OSS project with commerical support from AWS, Azure and others

						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### PGVector Example
							```sql wide-code
							create table documents (
								id bigint generated always as identity primary key,
								body_text text not null
							);

							-- populate with some document data

							create extension vector;

							create table document_embeddings (
								id bigint generated always as identity primary key,
								document_id bigint not null,
								position int not null,
								length int not null,
								embedding vector(1536) not null
							);
							```
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### PGVector Example Continued
							```sql wide-code
							-- generate embeddings for the documents

							insert into document_embeddings (document_id, position, length, embedding)
							select d.id, e.position, e.length, e.embedding 
							from documents d, 
							lateral generate_embeddings(
								body_text,
								model=>'BAAI/bge-large-en-v1.5',
								chunking_strategy=>'{"strategy": "sentences"}'::jsonb
							) e;

							-- create indexes
							create index on document_embeddings using ivfflat (embedding vector_l2_ops) with (lists = 100);
							-- or maybe
							create index on document_embeddings using hnsw (embedding vector_l2_ops);
							```
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### PGVector Example Continued
							```sql wide-code
							-- run a semantic search for relevant sentences
							with nearest_chunks as (
								select de.document_id, de.position, de.length
								from document_embeddings de
								order by de.embedding <=> generate_embedding(   -- This is the ANN search
									'Quick red foxes', 
									model=>'BAAI/bge-large-en-v1.5'
								) 
								limit 10
							)
							select substring(d.body_text from nc.position for nc.length) as chunk_text
							from nearest_chunks nc
							inner join documents d on nc.document_id = d.id
							```


						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Advantages of PGVector + PostgreSQL as a Vector DB for rag

							- <!-- .element: class="fragment" --> PostgreSQL is <span class="highlight-bold">mature</span>, <span class="highlight-blue-bold"></span>multi-purpose</span> database

							- <!-- .element: class="fragment" --> Data is all stored in one places

							- <!-- .element: class="fragment" --> No need for complex <span class="highlight-blue-bold">ETL</span> pipelines or <span class="highlight-red-bold">synchronisation</span> issues


						</textarea>
					</section>
				</section>




				<!-- More Practical considerations -->
				<section>
					<section data-markdown>
						<textarea data-template>
							# Cost Challenges for RAG Systems
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## Storing Embeddings 
							
							- <!-- .element: class="fragment" --> <span class="highlight-red-bold">Can be expensive to store!</span>
							
							- <!-- .element: class="fragment" --> At 300 words, 20 sentences per page
							
							- <!-- .element: class="fragment" --> Original text @ 5 characters per word would be 300*5*4 bytes = <span class="highlight-blue-bold">6KB per page</span>

							- <!-- .element: class="fragment" --> Embeddings of 1536 dims at sentence level is 1536 dims*4 bytes*20 = <span class="highlight-red-bold">122KB per page</span>
							
							- <!-- .element: class="fragment" --> This is before any indexes!
							
							- <!-- .element: class="fragment" --> And we may want <span class="highlight-bold">multiple search indexes</span> to cover different use cases (page/sentence/paragraph etc.)

							- <!-- .element: class="fragment" --> <span class="emoji">&#128184;</span>
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## New Embedding Approaches 
							- <!-- .element: class="fragment" --> <span class="highlight-blue-bold">Matryoshka Representation Learning</span>
								- <!-- .element: class="fragment" --> Nested embedding representation technique
								- <!-- .element: class="fragment" --> Allows for adaptive indexing of different granularities for <span class="highlight-bold">coarse/fine search</span>
								- <!-- .element: class="fragment" --> Or simply stored a fraction of the embedding

							- <!-- .element: class="fragment" --> <span class="highlight-orange-bold">Binary Quantization</span>
								- <!-- .element: class="fragment" --> Converts continuous embeddings to <span class="highlight-bold">binary format</span>
								- <!-- .element: class="fragment" --> Significantly reduces storage requirements
								- <!-- .element: class="fragment" --> Surprisingly <span class="highlight-blue-bold">low impact on search quality</span>
								- <!-- .element: class="fragment" --> Can significantly reduce costs
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### In The News

							- https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-october-updates-nearly-100x-compression-with/ba-p/4265447

							- Microsoft Azure AI Search introduced <span class="highlight-blue-bold">Matryoshka Representation Learning</span> and <span class="highlight-orange-bold">Binary Quantization</span> at the start of October

							- OpenAI text-embedding-3-* models are trained with MRL
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### Matryoshka Representation Learning
							<img src="./images/mrl.jpg" alt="mrl" height="200">

							- Named after Russian dolls

							- Trained so that bits in the embedding contain more information in the lower bits

							- E.g. take a 3096 dim embedding and cut half the bits to <span class="highlight-blue-bold">halve</span> the cost</span>
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							## Binary Quantization

							- A somewhat surprise finding
							- Converting <span class="highlight-orange-bold">32</span> bit floats to <span class="highlight-orange-bold">1</span> bit dimensions has a smallish impact on search quality
							- But storage costs are reduced by a factor of <span class="highlight-blue-bold">32!</span>
							- Most embedding models can be quantized but natively trained ones are better
						</textarea>
					</section>
					<section>
						<h2>Binary Quantization Process</h2>
						<img src="./images/binaryquantization.svg" alt="binaryQuantization">
					</section>
					<section data-markdown>
						<textarea data-template>
							### Binary Vector ANN Search
							- <!-- .element: class="fragment" --> Existing ANN approaches can be used (IVFFlat, HNSW)

							- <!-- .element: class="fragment" --> <span class="highlight-blue-bold">L1 distance</span> aka <span class="highlight-orange-bold">Manhattan distance</span> is equivalent to the <span class="highlight-blue-bold">Hamming distance</span>

							- <!-- .element: class="fragment" --> Hamming distance search has efficient algorithms (e.g. BK-Trees. VP-Trees)

							- <!-- .element: class="fragment" --> Coming soon (I hope) to <span class="highlight-bold">PGVector</span>
						</textarea>
					</section>
				</section>
				<!-- Q&A -->
				<section>
					<section data-markdown>
						<textarea data-template>
							# Conclusions
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
						### Conclusions

						- <!-- .element: class="fragment" --> <span class="highlight-orange-bold">Vector search</span> is a powerful technique for enhancing <span class="highlight-bold">RAG</span> systems

						- <!-- .element: class="fragment" --> But designing a good approach is not a "one size fits all" solution

						- <!-- .element: class="fragment" --> New embedding techniques like <span class="highlight-blue-bold">MRL</span> and <span class="highlight-orange-bold">BQ</span> can help with cost challenges

						- <!-- .element: class="fragment" --> <span class="highlight-bold">PGVector</span> is a great option for building <span class="highlight-bold">RAG</span> systems
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							# Q&A
	
							Thank you for your attention!
							Any questions?
						</textarea>
					</section>
				</section>


			</div>
			<div class="footer">
				<img src="images/logo.png" alt="Scileads Logo" class="logo">
				<span class="presentation-title">AI Use Cases In Action: Vector Search and RAG</span>
			</div>
		</div>

		<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/reveal.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/plugin/markdown/markdown.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/plugin/highlight/highlight.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/5.1.0/plugin/math/math.js"></script>
		<script>
			// Diagram definitions
			const diagrams = {
				autoencoderEmbedding: {
					type: 'tikz',
					content: `
\\documentclass[tikz,border=10pt]{standalone}
\\usepackage{tikz}
\\usetikzlibrary{shapes,arrows,positioning,fit}

\\begin{document}
\\begin{tikzpicture}[
    node distance=2cm,
    box/.style={rectangle,draw=black,thick,rounded corners,fill=blue!20,text width=2cm,align=center,font=\\sffamily},
    arrow/.style={->,>=stealth,thick},
    latent/.style={circle,draw=black,thick,fill=yellow!40,minimum size=1.5cm,font=\\sffamily\\bfseries},
    irregular/.style={trapezium,trapezium left angle=70,trapezium right angle=110,draw=black,thick,fill=blue!20,text width=2cm,align=center,font=\\sffamily}
]

% Nodes
\\node[box] (input) {Input Data};
\\node[irregular, right=of input, trapezium left angle=60, trapezium right angle=120] (encoder) {Encoder};
\\node[latent, right=of encoder] (latent) {Latent Vector};
\\node[irregular, right=of latent, trapezium left angle=120, trapezium right angle=60] (decoder) {Decoder};
\\node[box, right=of decoder] (output) {Reconstructed Data};

% Arrows
\\draw[arrow,red] (input) -- (encoder);
\\draw[arrow,green] (encoder) -- (latent);
\\draw[arrow,blue] (latent) -- (decoder);
\\draw[arrow,purple] (decoder) -- (output);

% Labels
\\node[font=\\small\\itshape,align=center,below=0.5cm of latent] (embedding) {Embedding};

\\end{tikzpicture}
\\end{document}
					`
				},
				transformerEmbedding: {
					type: 'graphviz',
					content: `
						digraph G {
							rankdir=LR;
							node [shape=box];
							Input -> Tokenizer;
							Tokenizer -> "Transformer Layers";
							"Transformer Layers" -> "Dense Vector";
							"Dense Vector" -> Output;
						}
					`
				},
				wordEmbeddings: {
					type: 'tikz',
					content: `
\\documentclass{article}
\\usepackage{tikz}
\\usepackage[active,tightpage]{preview}
\\PreviewEnvironment{tikzpicture}
\\setlength\\PreviewBorder{0.125pt}
\\begin{document}
\\begin{tikzpicture}[x={(1cm,0cm)},y={(0cm,1cm)},z={(-0.5cm,-0.5cm)}]
    % Grid lines
    \\foreach \\x in {0,1,...,4}
        \\foreach \\y in {0,1,...,4}
            \\foreach \\z in {0,1,...,4}
                \\draw[gray!30] (\\x,\\y,0) -- (\\x,\\y,4);
    \\foreach \\x in {0,1,...,4}
        \\foreach \\z in {0,1,...,4}
            \\draw[gray!30] (\\x,0,\\z) -- (\\x,4,\\z);
    \\foreach \\y in {0,1,...,4}
        \\foreach \\z in {0,1,...,4}
            \\draw[gray!30] (0,\\y,\\z) -- (4,\\y,\\z);

    % Axes
    \\draw[->] (0,0,0) -- (4,0,0) node[right]{x};
    \\draw[->] (0,0,0) -- (0,4,0) node[above]{y};
    \\draw[->] (0,0,0) -- (0,0,4) node[below left]{z};
    
    % Points
    \\node[circle,fill=red,inner sep=2pt] (cat) at (1,3,2) {};
    \\node[circle,fill=blue,inner sep=2pt] (dog) at (1.5,2.5,2.5) {};
    \\node[circle,fill=green,inner sep=2pt] (monday) at (3,1,1) {};
    \\node[circle,fill=orange,inner sep=2pt] (tuesday) at (3.5,1.5,1) {};
    
    % Lines from origin to points
    \\draw[red, dashed] (0,0,0) -- (cat);
    \\draw[blue, dashed] (0,0,0) -- (dog);
    \\draw[green, dashed] (0,0,0) -- (monday);
    \\draw[orange, dashed] (0,0,0) -- (tuesday);
    
    % Labels
    \\node[above right] at (cat) {cat};
    \\node[above right] at (dog) {dog};
    \\node[above right] at (monday) {monday};
    \\node[above right] at (tuesday) {tuesday};
\\end{tikzpicture}
\\end{document}
					`
				},
				semanticSearch: {
					type: 'mermaid',
					content: `
						graph TD
						A[User Query] --> B[Embedding Generation]
						B --> C[Vector Search]
						C --> D[Retrieve Similar Documents]
						D --> E[Rank Results]
						E --> F[Present to User]
					`
				},
				imageSimilarity: {
					type: 'mermaid',
					content: `
						graph TD
						A[Input Image] --> B[Feature Extraction]
						B --> C[Embedding Generation]
						C --> D[Vector Search]
						D --> E[Retrieve Similar Images]
						E --> F[Present Results]
					`
				},
				ragOverview: {
					type: 'mermaid',
					content: `
						graph TD
						A[User Query] --> B[Embedding Generation]
						B --> C[Vector Search]
						C --> D[Retrieve Relevant Documents]
						D --> E[Augment LLM Prompt]
						E --> F[Generate Response]
						F --> G[Present to User]
					`
				},
				ivfFlat: {
					type: 'mermaid',
					content: `
						graph TD
						A[Vector Space] --> B[Clustering]
						B --> C[Inverted Index]
						C --> D[Search within Relevant Clusters]
					`
				},
				hnsw: {
					type: 'mermaid',
					content: `
						graph TD
						A[Multi-layer Graph] --> B[Entry Point]
						B --> C[Navigate Layers]
						C --> D[Refine Search]
						D --> E[Nearest Neighbors]
					`
				},
				diskANN: {
					type: 'mermaid',
					content: `
						graph TD
						A[Large Dataset] --> B[Graph-based Index]
						B --> C[Disk Storage]
						C --> D[In-memory Graph]
						D --> E[Efficient Search]
					`
				},
				matryoshka: {
					type: 'mermaid',
					content: `
						graph TD
						A[Full Embedding] --> B[Nested Subsets]
						B --> C[Adaptive Search]
						C --> D[Multi-scale Similarity]
					`
				},
				binaryQuantization: {
					type: 'mermaid',
					content: `
						graph TD
						A[Continuous Embedding] --> B[Thresholding]
						B --> C[Binary Representation]
						C --> D[Compact Storage]
						D --> E[Fast Similarity Computation]
					`
				}
			};

			// Function to encode diagram to URL-safe string
			// function encodeKrokiDiagram(content) {
			// 	const encoder = new TextEncoder();
			// 	const data = encoder.encode(content);
			// 	const compressed = pako.deflate(data, { level: 9 });
			// 	return btoa(String.fromCharCode.apply(null, compressed))
			// 		.replace(/\+/g, '-')
			// 		.replace(/\//g, '_');
			// }

			Reveal.initialize({
				hash: true,
				plugins: [ RevealMarkdown, RevealHighlight, RevealMath.KaTeX ],
				slideNumber: 'c/t',
				transition: 'slide',
				autoAnimateDuration: 0.8,
				autoAnimateEasing: 'ease-in-out',
				autoAnimateUnmatched: false,
				fragmentInURL: true,
				embedded: false,
				help: true,
				center: true,
				touch: true,
				loop: false,
				rtl: false,
				navigationMode: 'default',
				shuffle: false,
				mouseWheel: false,
				display: 'block'
			});

		</script>
	</body>
</html>
